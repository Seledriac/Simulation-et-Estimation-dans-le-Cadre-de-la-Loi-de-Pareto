%% LyX 2.3.6.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{stackrel}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}

\part*{2 Partie théorique : étude mathématique}

\subsection*{2.1/ Etude probabiliste}

1. Soient $\alpha$ et $\theta$ deux réels strictements positifs.
Déterminer le réel $C$ pour que la fonction : $x\rightarrow\begin{cases}
\frac{C}{x^{\alpha+1}} & si\ x\geq\theta\\
0 & sinon.
\end{cases}$ soit une densité de probabilité.

On sait que $f$ est une densité si :
\begin{align*}
\int_{-\infty}^{+\infty}f(t)dt & =1\Leftrightarrow\int_{\theta}^{+\infty}\frac{C}{x^{\alpha+1}}dx=1\\
\Leftrightarrow & C\int_{\theta}^{+\infty}\frac{1}{x^{\alpha+1}}dx=1\\
\Leftrightarrow & C[\frac{-1}{\alpha x^{\alpha}}]_{\theta}^{+\infty}=1\\
\Leftrightarrow & \frac{C}{\alpha\theta^{\alpha}}=1\\
\Leftrightarrow & C=\alpha\theta^{\alpha}
\end{align*}
On obtient que :$f(x)=\begin{cases}
\frac{\alpha}{\theta}(\frac{\theta}{x})^{\alpha+1} & si\ x\geq\theta\\
0 & sinon
\end{cases}$\\
Dans toute la suite, on notera cette densité $f$.\\
\\
\textbf{Définition} : On dit que la variable aléatoire X suit une
loi de Pareto de paramètres $\alpha$ et $\theta$ si et seulement
si sa densité est $f.$ On notera alors $X\sim P(\alpha,\theta).$\\
\\
Dans ce qui suit, X désignera une variable aléatoire de loi $P(\alpha,\theta)$\\
\\
2. Dans quelle contexte cette loi est-elle utilisée? Donner des exemples
de modélisation avec une telle loi.\\

Après quelques recherches, on apprend que cette loi est utilisé en
économie principalement. Elle est néanmoins très discutée car beaucoup
la trouvent trop imprécise. A l'aide de la loi de Pareto on peut notemment
modeliser de la gestion de stock, de ventes, de projet ou de production.
On appelle aussi cette loi la loi des 80-20 car elle permet d'observer
que 80\% des effets sont le produit de seulement 20\% des causes.
En effet, si on modélise un problème comme étant causé par les valeurs
que peuvent prendre X, on voit qu'après $\theta$, approximativement
20\% des valeurs de cause constituent 80\% de l'intégrale de la densité
de probabilité du problème (voir figure \ref{fig:schema-densite}).
On peut généraliser ce raisonnement pour différentes valeurs de $\alpha$
(on a alors des proportions différentes de 80-20).\\
\\
\begin{figure}
\caption{Densité de loi de Pareto schématisée\label{fig:schema-densite}}

\centering{}\includegraphics{/home/tboumba/Bureau/pareto}
\end{figure}
\\
3. A quelle condition la variable X posède-t-elle une espérance ?
une variance ? On se placera sous cette dernière condition dans la
suite.
\begin{itemize}
\item X admet une espérance si :
\begin{align*}
 & \int_{-\infty}^{+\infty}|x|f(x)dx\ converge\\
\Leftrightarrow & \int_{\theta}^{+\infty}|x|\frac{\alpha}{\theta}(\frac{\theta}{x})^{\alpha+1}dx=\theta^{\alpha}\alpha\int_{\theta}^{+\infty}\frac{1}{x^{\alpha}}dx\ \ \ \ ,(\theta>0\Leftrightarrow|x|=x)\ converge\\
\Leftrightarrow\  & \alpha>1\ (Int\acute{e}grale\ de\ Riemann)
\end{align*}
\\
X admet une espérance si $\alpha>1$ et cette espérance vaut :\\
\[
E(X)=\int_{-\infty}^{+\infty}xf(x)dx
\]
\item X admet une variance si X admet une espérance et si :
\begin{align*}
 & \int_{-\infty}^{+\infty}(x-E(X))^{2}f(x)dx\ converge\\
\Leftrightarrow & \int_{\theta}^{+\infty}x^{2}f(x)dx+E(X)^{2}\int_{\theta}^{+\infty}f(x)dx-2E(X)\underset{E(X)}{\underbrace{\int_{\theta}^{+\infty}xf(x)dx}}=\int_{\theta}^{+\infty}x^{2}f(x)dx-E(X)^{2}\ converge\\
\Leftrightarrow & \int_{\theta}^{+\infty}x^{2}f(x)dx\ converge\\
\Leftrightarrow & \theta^{\alpha}\alpha\int_{\theta}^{+\infty}\frac{1}{x^{\alpha-1}}\ converge\\
\Leftrightarrow & \ \alpha-1>1\ (Int\acute{e}grale\ de\ Riemman)\\
\Leftrightarrow & \,\alpha>2
\end{align*}
\\
X admet une variance si $\alpha>2$ et cette variance vaut :
\[
V(X)=\int_{-\infty}^{+\infty}(x-E(x))^{2}f(x)dx=E(X^{2})-E(X)^{2}
\]
\end{itemize}
4. Calculer l'espérance et la variance de X.
\begin{itemize}
\item Calcul de l'espérance :
\begin{align*}
E(X) & =\theta^{\alpha}\alpha\int_{\theta}^{+\infty}\frac{1}{x^{\alpha}}dx\\
= & \theta^{\alpha}\alpha[\frac{-1}{(\alpha-1)x^{\alpha-1}}]_{\theta}^{+\infty}\\
= & \theta^{\alpha}\alpha\frac{1}{(\alpha-1)\theta^{\alpha-1}}\\
= & \frac{\theta\alpha}{(\alpha-1)}
\end{align*}
\item Calcul de la variance :
\begin{align*}
V(x)= & E(X^{2})-E(X)^{2}\\
E(X^{2})= & \frac{\theta^{2}\alpha}{(\alpha-2)}\\
V(X)= & \frac{\theta^{2}\alpha}{(\alpha-2)}-\frac{\theta^{2}\alpha^{2}}{(\alpha-1)^{2}}\\
V(X)= & \frac{\theta^{2}\alpha(\alpha-1)^{2}-\theta^{2}\alpha^{2}(\alpha-2)}{(\alpha-1)^{2}(\alpha-2)}\\
V(X)= & \frac{\alpha\theta^{2}}{(\alpha-1)^{2}(\alpha-2)}
\end{align*}
\end{itemize}
5. Calculer la fonction de répartition F de X.

La fonction de répartition, notée $F_{X}(x)$ se calcule suivant la
formule suivante :

\begin{align*}
F_{X}(x)= & P(X\leq x)=\int_{-\infty}^{x}f(t)dt\\
= & \begin{cases}
0 & si\ x<\theta\\
\int_{\theta}^{x}\frac{\alpha}{\theta}(\frac{\theta}{t})^{\alpha+1}dt & si\ x\geq\theta
\end{cases}\\
= & \begin{cases}
0 & si\ x<\theta\\
\alpha\theta^{\alpha}[\frac{-1}{\alpha t^{\alpha}}]_{\theta}^{x}=\frac{\bcancel{\alpha\theta^{\alpha}}}{\bcancel{\alpha\theta^{\alpha}}}-\frac{\bcancel{\alpha}\theta^{\alpha}}{\bcancel{\alpha}x^{\alpha}} & si\ x\geq\theta
\end{cases}\\
= & \begin{cases}
0 & si\ x<\theta\\
1-(\frac{\theta}{x})^{\alpha} & si\ x\geq\theta
\end{cases}
\end{align*}
\\
6. Déterminer la fonction réciproque de F.

\begin{align*}
y= & 1-(\frac{\theta}{x})^{\alpha}\\
(\frac{\theta}{x})^{\alpha}= & 1-y\\
x^{\alpha}= & \frac{\theta^{\alpha}}{1-y}\\
x= & \frac{\theta}{(1-y)^{1/\alpha}}
\end{align*}

On a donc : 
\[
F^{-1}(x)=\frac{\theta}{(1-x)^{1/\alpha}}
\]


\subsection*{2.2/ Simulation d'une valeur d'une variable de loi de Pareto}

L\textquoteright objet de ce paragraphe est de proposer une méthode
de construction d\textquoteright une valeur d\textquoteright une variable
aléatoire de loi P($\alpha$,$\theta$).\\
\\
7. Expliquer de manière formelle comment simuler une valeur d\textquoteright une
variable aléatoire de loi P($\alpha$,$\theta$).

On va appliquer la méthode d'inversion. 

Soit X une variable réelle de loi continue et strictement croissante.
Alors si U est de loi uniforme sur {[}0,1{]}, la variable $F^{-1}(U)$
a la même loi que X.

Dans le cadre de la loi de Pareto :

\[
\begin{cases}
F(\theta^{-})=0\\
F(\theta)=1-(\frac{\theta}{\theta})^{\alpha}=0
\end{cases}
\]

Donc F est continue en $\theta$, de plus F est continue de manière
triviale sur $\mathbb{R}\backslash\{\theta\},$donc F est continue
sur $\mathbb{R}$.

F est également strictement croissante sur $[\theta,+\infty[$ de
manière évidente.

On peut donc appliquer la méthode d'inversion à la loi de Pareto pour
en simuler une valeur $x$ en simulant une réalisation u de U et en
posant $x=F^{-1}(u)=\frac{\theta}{(1-u)^{1/\alpha}}$.

\subsection*{2.3/ Etude statistique}

On considère $n$ données réelles $x_{1},x_{2},...,x_{n}.$ On fait
l'hypothèse que ces données sont les réalisations de $n$ variables
aléatoires $X_{1},X_{2},...,X_{n},$indépendantes et de même loi $P(\alpha,\theta=1).$
On suppose que le paramètre $\alpha$ est inconu et on s'intéresse
au problème de son estimation par la méthode du maximum de vraisemblance.\\
\\
8. Ecrire la log-vraisemblance de l\textquoteright échantillon et
trouver l\textquoteright estimation du maximum de vraisemblance de
$\alpha$. En déduire alors l\textquoteright estimateur du maximum
de vraisemblance du paramètre $\alpha$. On le notera $\widehat{\alpha}_{n}$.\\
\\
\begin{align*}
L(x,\alpha)= & \prod_{j=1}^{n}\alpha(\frac{1}{x_{j}})^{\alpha+1}\\
log(L(x,\alpha))= & nln(\alpha)-\sum_{j=1}^{n}(\alpha+1)ln(x_{j})\\
= & nln(\alpha)-(\alpha+1)s_{n}\ avec\ s_{n}=\sum_{j=1}^{n}ln(x_{j})\\
t_{n}= & arg\ \underset{z\in[1,+\infty]}{max(LL(x,z))}\\
g(z)= & nln(z)-(z+1)s_{n}\\
g^{\prime}(z)= & \frac{n}{z}-s_{n}\\
g^{\prime\prime}(z)= & \frac{-n}{z^{2}}
\end{align*}
Donc $g^{\prime}(z)=0\Leftrightarrow z=\frac{n}{s_{n}}$ et $g^{\prime\prime}(z)<0.$
L'unique valeur $t_{n}$ qui maximise la vraisemblance sachant $x_{1},x_{2},...,x_{n}$
est donc égale à $t_{n}=\frac{n}{s_{n}}$\\
On peut alors construire un estimateur du maximum de vraisemblance
de $\alpha$ noté $\widehat{\alpha}_{n}$: $\widehat{\alpha}_{n}=\frac{n}{\sum_{j=1}^{n}log(X_{j})}$\\
9. Calculer $E[log(X_{1})]$ et $Var(log(X_{1})).$\\
\\
\begin{align*}
E(log(X_{1}))= & \int_{1}^{+\infty}\alpha ln(x)(\frac{1}{x}){}^{\alpha+1}dx\\
= & \alpha\int_{1}^{+\infty}ln(x)(\frac{1}{x})^{\alpha+1}dx\\
\text{On va procéder par intégration par parties :} & u(x)=ln(x),u^{\prime}(x)=\frac{1}{x},v^{\prime}(x)=\frac{1}{x^{\alpha+1}},v(x)=\frac{-1}{\alpha x^{\alpha}}\\
\int_{1}^{+\infty}ln(x)(\frac{1}{x})^{\alpha+1}dx= & [\frac{-ln(x)}{\alpha x^{\alpha}}]_{1}^{+\infty}+\int_{1}^{+\infty}\frac{1}{\alpha x^{\alpha+1}}dx\\
= & 0+[\frac{-1}{\alpha^{2}x^{\alpha}}]_{1}^{+\infty}=\frac{1}{\alpha^{2}}\\
\text{donc \ensuremath{E(log(X_{1}))=}} & \frac{1}{\alpha}
\end{align*}
Calcul de $Var(log(X_{1}))$ :
\begin{align*}
Var(log(X_{1}))= & E((log(X_{1}))^{2})-E(log(X_{1}))^{2}\\
E((log(X_{1}))^{2})= & \alpha\int_{1}^{+\infty}ln^{2}(x)(\frac{1}{x})^{\alpha+1}dx\\
= & \underset{0}{\alpha(\underbrace{[\frac{-ln^{2}(x)}{\alpha x^{\alpha}}]_{1}^{+\infty}}}+\int_{1}^{+\infty}\frac{2ln(x)}{\alpha x^{\alpha+1}}dx)\\
= & \frac{2}{\alpha^{2}}\\
Var(log(X_{1}))= & \frac{1}{\alpha^{2}}
\end{align*}
\\
\\
10. Proposer un estimateur sans biais et convergent du paramètre $\beta=\frac{1}{\alpha}$.
On notera cet estimateur $B_{n}$et $b_{n}$sa réalisation.\\
\\
\[
B_{n}=\frac{1}{n}\sum_{i=1}^{+\infty}log(X_{i})
\]
\\
$B_{n}$ est un estimateur du paramètre $\beta=\frac{1}{\alpha}$
sans biais et convergent. En effet, $E(log(X_{1}))=\frac{1}{\alpha}$
donc par linéarité de l'espérance, on a\\
\\
\[
E(B_{n})=\frac{\bcancel{n}}{\bcancel{n}}\beta=\beta
\]
\\
De même, puisque les variables $X_{i}$ indépendantes, on a la linéarité
de la variance de leur somme, donc\\
\\
\[
Var(B_{n})=\frac{n}{n^{2}}\beta^{2}\rightarrow0
\]
\\
11. Enoncer le TLC que satisfait cet estimateur.\\
\\
$E(log(X_{1})^{2})<\infty$ car la variance de $log(X_{i})$ existe.
Cette variable admet donc un momet d'ordre 2. $B_{n}$vérifie donc
le Théorème limite centrale suivant :\\
\[
\sqrt{n}(\underset{B_{n}}{\underbrace{\frac{1}{n}\sum_{j=1}^{n}log(X_{i})}}-E(log(X_{i})))\stackrel[n\rightarrow\infty]{\mathcal{L}}{\rightarrow}N(0,Var(log(X_{i})))
\]

\[
\sqrt{n}(B_{n}-\beta)\stackrel[n\rightarrow\infty]{\mathcal{L}}{\rightarrow}N(0,\beta^{2})
\]
\\
12. Proposer un estimateur du paramètre $\alpha$. On le notera $A_{n}$.\\
\\
\[
A_{n}=B_{n}^{-1}=\frac{n}{\sum_{i=1}^{n}log(X_{i})}=\widehat{\alpha_{n}}
\]
\\
\\
13. En déduire la convergence de $A_{n}$vers $\alpha$ et énoncer
le TLC que satisfait $A_{n}.$

D'après la loi forte des grands nombres, on a :

\begin{align*}
B_{n} & \stackrel[n\rightarrow\infty]{p.s}{\rightarrow}\beta\\
\text{donc }\frac{1}{B_{n}}=A_{n} & \stackrel[n\rightarrow\infty]{p.s}{\rightarrow}\frac{1}{\beta}=\alpha
\end{align*}

$A_{n}$converge donc bien vers $\alpha$.

On a :

\[
\sqrt{n}(B_{n}-\beta)\stackrel[n\rightarrow\infty]{\mathcal{L}}{\rightarrow}N(0,\beta^{2})
\]

On utilise la delta-méthode en utilisant $g(x)=\frac{1}{x}$, dérivable
en $\alpha$ de manière évidente ($\alpha>0$)

\[
\sqrt{n}(\frac{1}{B_{n}}-\frac{1}{\beta})\stackrel[n\rightarrow\infty]{\mathcal{L}}{\rightarrow}N(0,(-\frac{1}{\beta^{2}})^{2}\beta^{2})
\]

On obtient donc finalement :

\[
\sqrt{n}(A_{n}-\alpha)\stackrel[n\rightarrow\infty]{\mathcal{L}}{\rightarrow}N(0,\alpha^{2})
\]


\subsection*{2.4 Récapitulatif de la partie mathématique}

\begin{tabular}{|c|c|c|}
\hline 
Fonction de densité & $f(x)$ & $\begin{cases}
\frac{\alpha}{\theta}(\frac{\theta}{x})^{\alpha+1} & si\ x\geq\theta\\
0 & sinon
\end{cases}$\tabularnewline
\hline 
Fonction de répartition & $F(x)$ & $\begin{cases}
0 & si\ x<\theta\\
1-(\frac{\theta}{x})^{\alpha} & si\ x\geq\theta
\end{cases}$\tabularnewline
\hline 
Fonction inverse de la fonction de répartition & $F^{-1}(x)$ & 

$F^{-1}(x)=\frac{\theta}{(1-x)^{1/\alpha}}$


\tabularnewline
\hline 
Esperance & $E(X)$ & $\frac{\theta\alpha}{(\alpha-1)}$ $(\alpha>1)$\tabularnewline
\hline 
Variance & $V(x)$ & $\frac{\alpha\theta^{2}}{(\alpha-1)^{2}(\alpha-2)}$$(\alpha>2)$\tabularnewline
\hline 
Estimateur du paramètre $\alpha$ & $A_{n}$ & $\frac{n}{\sum_{i=1}^{n}log(X_{i})}$\tabularnewline
\hline 
\end{tabular}
\end{document}
